{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_deep_learning_basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oshqJ7tTpO32"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://deeplearning.mit.edu\">\n",
        "        <img src=\"https://deeplearning.mit.edu/files/images/github/icon_mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://colab.research.google.com/github/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb\">\n",
        "        <img src=\"https://deeplearning.mit.edu/files/images/github/icon_google_colab.png\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_deep_learning_basics/deep_learning_basics.ipynb\">\n",
        "        <img src=\"https://deeplearning.mit.edu/files/images/github/icon_github.png\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" align=\"center\" href=\"https://www.youtube.com/watch?v=O5xeyoRL95U&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf\">\n",
        "        <img src=\"https://deeplearning.mit.edu/files/images/github/icon_youtube.png\" style=\"padding-bottom:5px;\" />Watch YouTube Videos</a></td>\n",
        "<!--   <td><a target=\"_blank\" href=\"link\">\n",
        "        <img src=\"image\" />text</a></td> -->\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y1MP6IN9pO34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc00c244-7af3-41fc-e98d-195f14dcedf7"
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, LeakyReLU, Embedding, Concatenate\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Commonly used modules\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "from math import ceil, sqrt\n",
        "from ast import literal_eval\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import IPython\n",
        "from six.moves import urllib\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DawuYjfnKb0B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "9b50d2bd-458a-4415-eba6-a5d95482db5d"
      },
      "source": [
        "hm_train_stats"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hm_c_AGE</th>\n",
              "      <th>hm_c_AST/TOV</th>\n",
              "      <th>hm_c_AST_PCT</th>\n",
              "      <th>hm_c_AST_PP</th>\n",
              "      <th>hm_c_AST_TOT</th>\n",
              "      <th>hm_c_BLK_PP</th>\n",
              "      <th>hm_c_BLK_TOT</th>\n",
              "      <th>hm_c_DREB_PCT</th>\n",
              "      <th>hm_c_DREB_PP</th>\n",
              "      <th>hm_c_DREB_TOT</th>\n",
              "      <th>hm_c_FG2A_PP</th>\n",
              "      <th>hm_c_FG2A_TOT</th>\n",
              "      <th>hm_c_FG2M_PP</th>\n",
              "      <th>hm_c_FG2M_TOT</th>\n",
              "      <th>hm_c_FG2_PCT</th>\n",
              "      <th>hm_c_FG3A_PP</th>\n",
              "      <th>hm_c_FG3A_TOT</th>\n",
              "      <th>hm_c_FG3M_PP</th>\n",
              "      <th>hm_c_FG3M_TOT</th>\n",
              "      <th>hm_c_FG3_FREQ</th>\n",
              "      <th>hm_c_FG3_PCT</th>\n",
              "      <th>hm_c_FTA_PP</th>\n",
              "      <th>hm_c_FTA_TOT</th>\n",
              "      <th>hm_c_FTM_PP</th>\n",
              "      <th>hm_c_FTM_TOT</th>\n",
              "      <th>hm_c_FT_PCT</th>\n",
              "      <th>hm_c_HEIGHT</th>\n",
              "      <th>hm_c_L</th>\n",
              "      <th>hm_c_MIN_TOT</th>\n",
              "      <th>hm_c_NET_RATING</th>\n",
              "      <th>hm_c_OREB_PCT</th>\n",
              "      <th>hm_c_OREB_PP</th>\n",
              "      <th>hm_c_OREB_TOT</th>\n",
              "      <th>hm_c_PFD_PP</th>\n",
              "      <th>hm_c_PFD_TOT</th>\n",
              "      <th>hm_c_PF_PP</th>\n",
              "      <th>hm_c_PF_TOT</th>\n",
              "      <th>hm_c_PLUS_MINUS</th>\n",
              "      <th>hm_c_STL_PP</th>\n",
              "      <th>hm_c_STL_TOT</th>\n",
              "      <th>...</th>\n",
              "      <th>hm_g_2_DREB_PP</th>\n",
              "      <th>hm_g_2_DREB_TOT</th>\n",
              "      <th>hm_g_2_FG2A_PP</th>\n",
              "      <th>hm_g_2_FG2A_TOT</th>\n",
              "      <th>hm_g_2_FG2M_PP</th>\n",
              "      <th>hm_g_2_FG2M_TOT</th>\n",
              "      <th>hm_g_2_FG2_PCT</th>\n",
              "      <th>hm_g_2_FG3A_PP</th>\n",
              "      <th>hm_g_2_FG3A_TOT</th>\n",
              "      <th>hm_g_2_FG3M_PP</th>\n",
              "      <th>hm_g_2_FG3M_TOT</th>\n",
              "      <th>hm_g_2_FG3_FREQ</th>\n",
              "      <th>hm_g_2_FG3_PCT</th>\n",
              "      <th>hm_g_2_FTA_PP</th>\n",
              "      <th>hm_g_2_FTA_TOT</th>\n",
              "      <th>hm_g_2_FTM_PP</th>\n",
              "      <th>hm_g_2_FTM_TOT</th>\n",
              "      <th>hm_g_2_FT_PCT</th>\n",
              "      <th>hm_g_2_HEIGHT</th>\n",
              "      <th>hm_g_2_L</th>\n",
              "      <th>hm_g_2_MIN_TOT</th>\n",
              "      <th>hm_g_2_NET_RATING</th>\n",
              "      <th>hm_g_2_OREB_PCT</th>\n",
              "      <th>hm_g_2_OREB_PP</th>\n",
              "      <th>hm_g_2_OREB_TOT</th>\n",
              "      <th>hm_g_2_PFD_PP</th>\n",
              "      <th>hm_g_2_PFD_TOT</th>\n",
              "      <th>hm_g_2_PF_PP</th>\n",
              "      <th>hm_g_2_PF_TOT</th>\n",
              "      <th>hm_g_2_PLUS_MINUS</th>\n",
              "      <th>hm_g_2_STL_PP</th>\n",
              "      <th>hm_g_2_STL_TOT</th>\n",
              "      <th>hm_g_2_TOV_PP</th>\n",
              "      <th>hm_g_2_TOV_TOT</th>\n",
              "      <th>hm_g_2_TS_PCT</th>\n",
              "      <th>hm_g_2_USG_PCT</th>\n",
              "      <th>hm_g_2_W</th>\n",
              "      <th>hm_g_2_W_PCT</th>\n",
              "      <th>hm_W_PCT</th>\n",
              "      <th>hm_GP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12664</th>\n",
              "      <td>28.0</td>\n",
              "      <td>0.775862</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.019</td>\n",
              "      <td>45</td>\n",
              "      <td>0.022</td>\n",
              "      <td>52</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.064</td>\n",
              "      <td>153</td>\n",
              "      <td>0.164</td>\n",
              "      <td>394</td>\n",
              "      <td>0.080</td>\n",
              "      <td>192</td>\n",
              "      <td>0.487310</td>\n",
              "      <td>0.045</td>\n",
              "      <td>108</td>\n",
              "      <td>0.012</td>\n",
              "      <td>30</td>\n",
              "      <td>0.215139</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.042</td>\n",
              "      <td>102</td>\n",
              "      <td>0.035</td>\n",
              "      <td>84</td>\n",
              "      <td>0.824</td>\n",
              "      <td>84</td>\n",
              "      <td>27</td>\n",
              "      <td>1257.120000</td>\n",
              "      <td>-6.1</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.029</td>\n",
              "      <td>69</td>\n",
              "      <td>0.044</td>\n",
              "      <td>107</td>\n",
              "      <td>0.042</td>\n",
              "      <td>100</td>\n",
              "      <td>-123</td>\n",
              "      <td>0.006</td>\n",
              "      <td>14</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037</td>\n",
              "      <td>143</td>\n",
              "      <td>0.107</td>\n",
              "      <td>413</td>\n",
              "      <td>0.046</td>\n",
              "      <td>178</td>\n",
              "      <td>0.430993</td>\n",
              "      <td>0.051</td>\n",
              "      <td>195</td>\n",
              "      <td>0.016</td>\n",
              "      <td>62</td>\n",
              "      <td>0.320724</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.032</td>\n",
              "      <td>122</td>\n",
              "      <td>0.023</td>\n",
              "      <td>88</td>\n",
              "      <td>0.721</td>\n",
              "      <td>73</td>\n",
              "      <td>34</td>\n",
              "      <td>2017.310000</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.014</td>\n",
              "      <td>54</td>\n",
              "      <td>0.033</td>\n",
              "      <td>128</td>\n",
              "      <td>0.041</td>\n",
              "      <td>157</td>\n",
              "      <td>25</td>\n",
              "      <td>0.020</td>\n",
              "      <td>78</td>\n",
              "      <td>0.033</td>\n",
              "      <td>129</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.181</td>\n",
              "      <td>31</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.405</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5965</th>\n",
              "      <td>35.0</td>\n",
              "      <td>1.365591</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.034</td>\n",
              "      <td>127</td>\n",
              "      <td>0.035</td>\n",
              "      <td>132</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.141</td>\n",
              "      <td>529</td>\n",
              "      <td>0.131</td>\n",
              "      <td>492</td>\n",
              "      <td>0.067</td>\n",
              "      <td>253</td>\n",
              "      <td>0.514228</td>\n",
              "      <td>0.001</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.048</td>\n",
              "      <td>182</td>\n",
              "      <td>0.035</td>\n",
              "      <td>132</td>\n",
              "      <td>0.725</td>\n",
              "      <td>83</td>\n",
              "      <td>46</td>\n",
              "      <td>1925.365000</td>\n",
              "      <td>-5.4</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.043</td>\n",
              "      <td>161</td>\n",
              "      <td>0.037</td>\n",
              "      <td>141</td>\n",
              "      <td>0.034</td>\n",
              "      <td>129</td>\n",
              "      <td>-205</td>\n",
              "      <td>0.014</td>\n",
              "      <td>51</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030</td>\n",
              "      <td>156</td>\n",
              "      <td>0.108</td>\n",
              "      <td>564</td>\n",
              "      <td>0.053</td>\n",
              "      <td>280</td>\n",
              "      <td>0.496454</td>\n",
              "      <td>0.064</td>\n",
              "      <td>337</td>\n",
              "      <td>0.025</td>\n",
              "      <td>131</td>\n",
              "      <td>0.374029</td>\n",
              "      <td>0.389</td>\n",
              "      <td>0.067</td>\n",
              "      <td>350</td>\n",
              "      <td>0.057</td>\n",
              "      <td>299</td>\n",
              "      <td>0.854</td>\n",
              "      <td>75</td>\n",
              "      <td>59</td>\n",
              "      <td>2676.651667</td>\n",
              "      <td>-8.7</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.009</td>\n",
              "      <td>45</td>\n",
              "      <td>0.059</td>\n",
              "      <td>308</td>\n",
              "      <td>0.033</td>\n",
              "      <td>172</td>\n",
              "      <td>-459</td>\n",
              "      <td>0.015</td>\n",
              "      <td>78</td>\n",
              "      <td>0.031</td>\n",
              "      <td>164</td>\n",
              "      <td>0.593</td>\n",
              "      <td>0.205</td>\n",
              "      <td>19</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.238</td>\n",
              "      <td>80.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13739</th>\n",
              "      <td>21.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.019</td>\n",
              "      <td>66</td>\n",
              "      <td>0.024</td>\n",
              "      <td>86</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.092</td>\n",
              "      <td>324</td>\n",
              "      <td>0.112</td>\n",
              "      <td>397</td>\n",
              "      <td>0.062</td>\n",
              "      <td>217</td>\n",
              "      <td>0.546599</td>\n",
              "      <td>0.001</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.058</td>\n",
              "      <td>205</td>\n",
              "      <td>0.029</td>\n",
              "      <td>103</td>\n",
              "      <td>0.502</td>\n",
              "      <td>84</td>\n",
              "      <td>33</td>\n",
              "      <td>1770.550000</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.056</td>\n",
              "      <td>199</td>\n",
              "      <td>0.053</td>\n",
              "      <td>186</td>\n",
              "      <td>0.063</td>\n",
              "      <td>222</td>\n",
              "      <td>46</td>\n",
              "      <td>0.011</td>\n",
              "      <td>38</td>\n",
              "      <td>...</td>\n",
              "      <td>0.076</td>\n",
              "      <td>364</td>\n",
              "      <td>0.248</td>\n",
              "      <td>1183</td>\n",
              "      <td>0.113</td>\n",
              "      <td>541</td>\n",
              "      <td>0.457312</td>\n",
              "      <td>0.060</td>\n",
              "      <td>288</td>\n",
              "      <td>0.018</td>\n",
              "      <td>86</td>\n",
              "      <td>0.195785</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.137</td>\n",
              "      <td>654</td>\n",
              "      <td>0.114</td>\n",
              "      <td>546</td>\n",
              "      <td>0.835</td>\n",
              "      <td>75</td>\n",
              "      <td>27</td>\n",
              "      <td>2301.823333</td>\n",
              "      <td>4.7</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.026</td>\n",
              "      <td>124</td>\n",
              "      <td>0.093</td>\n",
              "      <td>443</td>\n",
              "      <td>0.039</td>\n",
              "      <td>184</td>\n",
              "      <td>247</td>\n",
              "      <td>0.029</td>\n",
              "      <td>140</td>\n",
              "      <td>0.061</td>\n",
              "      <td>293</td>\n",
              "      <td>0.536</td>\n",
              "      <td>0.368</td>\n",
              "      <td>40</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.487</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6591</th>\n",
              "      <td>34.0</td>\n",
              "      <td>1.542373</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.023</td>\n",
              "      <td>91</td>\n",
              "      <td>0.022</td>\n",
              "      <td>86</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.110</td>\n",
              "      <td>434</td>\n",
              "      <td>0.101</td>\n",
              "      <td>401</td>\n",
              "      <td>0.047</td>\n",
              "      <td>189</td>\n",
              "      <td>0.471322</td>\n",
              "      <td>0.081</td>\n",
              "      <td>319</td>\n",
              "      <td>0.029</td>\n",
              "      <td>113</td>\n",
              "      <td>0.443056</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.026</td>\n",
              "      <td>101</td>\n",
              "      <td>0.020</td>\n",
              "      <td>78</td>\n",
              "      <td>0.772</td>\n",
              "      <td>83</td>\n",
              "      <td>34</td>\n",
              "      <td>2122.505000</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.014</td>\n",
              "      <td>56</td>\n",
              "      <td>0.021</td>\n",
              "      <td>84</td>\n",
              "      <td>0.050</td>\n",
              "      <td>197</td>\n",
              "      <td>83</td>\n",
              "      <td>0.016</td>\n",
              "      <td>62</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>143</td>\n",
              "      <td>0.188</td>\n",
              "      <td>735</td>\n",
              "      <td>0.082</td>\n",
              "      <td>320</td>\n",
              "      <td>0.435374</td>\n",
              "      <td>0.025</td>\n",
              "      <td>99</td>\n",
              "      <td>0.007</td>\n",
              "      <td>28</td>\n",
              "      <td>0.118705</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.088</td>\n",
              "      <td>347</td>\n",
              "      <td>0.069</td>\n",
              "      <td>271</td>\n",
              "      <td>0.781</td>\n",
              "      <td>72</td>\n",
              "      <td>32</td>\n",
              "      <td>2092.430000</td>\n",
              "      <td>-1.5</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.008</td>\n",
              "      <td>30</td>\n",
              "      <td>0.081</td>\n",
              "      <td>316</td>\n",
              "      <td>0.022</td>\n",
              "      <td>86</td>\n",
              "      <td>-46</td>\n",
              "      <td>0.022</td>\n",
              "      <td>88</td>\n",
              "      <td>0.037</td>\n",
              "      <td>146</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.258</td>\n",
              "      <td>25</td>\n",
              "      <td>0.439</td>\n",
              "      <td>0.529</td>\n",
              "      <td>51.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10810</th>\n",
              "      <td>24.0</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.007</td>\n",
              "      <td>28</td>\n",
              "      <td>0.029</td>\n",
              "      <td>112</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.100</td>\n",
              "      <td>384</td>\n",
              "      <td>0.128</td>\n",
              "      <td>488</td>\n",
              "      <td>0.082</td>\n",
              "      <td>314</td>\n",
              "      <td>0.643443</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.065</td>\n",
              "      <td>249</td>\n",
              "      <td>0.025</td>\n",
              "      <td>96</td>\n",
              "      <td>0.386</td>\n",
              "      <td>83</td>\n",
              "      <td>26</td>\n",
              "      <td>2009.690000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.055</td>\n",
              "      <td>210</td>\n",
              "      <td>0.053</td>\n",
              "      <td>201</td>\n",
              "      <td>0.050</td>\n",
              "      <td>192</td>\n",
              "      <td>274</td>\n",
              "      <td>0.013</td>\n",
              "      <td>48</td>\n",
              "      <td>...</td>\n",
              "      <td>0.038</td>\n",
              "      <td>30</td>\n",
              "      <td>0.052</td>\n",
              "      <td>42</td>\n",
              "      <td>0.025</td>\n",
              "      <td>20</td>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.113</td>\n",
              "      <td>90</td>\n",
              "      <td>0.041</td>\n",
              "      <td>33</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.060</td>\n",
              "      <td>48</td>\n",
              "      <td>0.056</td>\n",
              "      <td>45</td>\n",
              "      <td>0.938</td>\n",
              "      <td>75</td>\n",
              "      <td>6</td>\n",
              "      <td>418.300000</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.003</td>\n",
              "      <td>2</td>\n",
              "      <td>0.046</td>\n",
              "      <td>37</td>\n",
              "      <td>0.031</td>\n",
              "      <td>25</td>\n",
              "      <td>46</td>\n",
              "      <td>0.015</td>\n",
              "      <td>12</td>\n",
              "      <td>0.033</td>\n",
              "      <td>26</td>\n",
              "      <td>0.601</td>\n",
              "      <td>0.194</td>\n",
              "      <td>16</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.687</td>\n",
              "      <td>67.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11823</th>\n",
              "      <td>31.0</td>\n",
              "      <td>1.944444</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.040</td>\n",
              "      <td>140</td>\n",
              "      <td>0.011</td>\n",
              "      <td>39</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.126</td>\n",
              "      <td>442</td>\n",
              "      <td>0.129</td>\n",
              "      <td>452</td>\n",
              "      <td>0.065</td>\n",
              "      <td>227</td>\n",
              "      <td>0.502212</td>\n",
              "      <td>0.002</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015251</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.038</td>\n",
              "      <td>135</td>\n",
              "      <td>0.026</td>\n",
              "      <td>92</td>\n",
              "      <td>0.681</td>\n",
              "      <td>82</td>\n",
              "      <td>40</td>\n",
              "      <td>1800.238333</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.053</td>\n",
              "      <td>187</td>\n",
              "      <td>0.043</td>\n",
              "      <td>152</td>\n",
              "      <td>0.047</td>\n",
              "      <td>164</td>\n",
              "      <td>63</td>\n",
              "      <td>0.020</td>\n",
              "      <td>69</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045</td>\n",
              "      <td>201</td>\n",
              "      <td>0.116</td>\n",
              "      <td>513</td>\n",
              "      <td>0.051</td>\n",
              "      <td>223</td>\n",
              "      <td>0.434698</td>\n",
              "      <td>0.042</td>\n",
              "      <td>185</td>\n",
              "      <td>0.014</td>\n",
              "      <td>63</td>\n",
              "      <td>0.265043</td>\n",
              "      <td>0.341</td>\n",
              "      <td>0.034</td>\n",
              "      <td>149</td>\n",
              "      <td>0.028</td>\n",
              "      <td>125</td>\n",
              "      <td>0.839</td>\n",
              "      <td>75</td>\n",
              "      <td>48</td>\n",
              "      <td>2252.110000</td>\n",
              "      <td>-5.6</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.005</td>\n",
              "      <td>22</td>\n",
              "      <td>0.035</td>\n",
              "      <td>156</td>\n",
              "      <td>0.030</td>\n",
              "      <td>133</td>\n",
              "      <td>-240</td>\n",
              "      <td>0.013</td>\n",
              "      <td>56</td>\n",
              "      <td>0.030</td>\n",
              "      <td>133</td>\n",
              "      <td>0.498</td>\n",
              "      <td>0.176</td>\n",
              "      <td>32</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.327</td>\n",
              "      <td>49.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15436</th>\n",
              "      <td>23.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.040</td>\n",
              "      <td>26</td>\n",
              "      <td>0.020</td>\n",
              "      <td>13</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.110</td>\n",
              "      <td>72</td>\n",
              "      <td>0.104</td>\n",
              "      <td>68</td>\n",
              "      <td>0.061</td>\n",
              "      <td>40</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.006</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.054</td>\n",
              "      <td>35</td>\n",
              "      <td>0.038</td>\n",
              "      <td>25</td>\n",
              "      <td>0.714</td>\n",
              "      <td>82</td>\n",
              "      <td>17</td>\n",
              "      <td>320.703333</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.046</td>\n",
              "      <td>30</td>\n",
              "      <td>0.034</td>\n",
              "      <td>22</td>\n",
              "      <td>0.047</td>\n",
              "      <td>31</td>\n",
              "      <td>53</td>\n",
              "      <td>0.009</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056</td>\n",
              "      <td>279</td>\n",
              "      <td>0.167</td>\n",
              "      <td>830</td>\n",
              "      <td>0.081</td>\n",
              "      <td>406</td>\n",
              "      <td>0.489157</td>\n",
              "      <td>0.041</td>\n",
              "      <td>205</td>\n",
              "      <td>0.013</td>\n",
              "      <td>64</td>\n",
              "      <td>0.198068</td>\n",
              "      <td>0.312</td>\n",
              "      <td>0.095</td>\n",
              "      <td>475</td>\n",
              "      <td>0.079</td>\n",
              "      <td>395</td>\n",
              "      <td>0.832</td>\n",
              "      <td>79</td>\n",
              "      <td>30</td>\n",
              "      <td>2473.625000</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.016</td>\n",
              "      <td>79</td>\n",
              "      <td>0.074</td>\n",
              "      <td>368</td>\n",
              "      <td>0.025</td>\n",
              "      <td>124</td>\n",
              "      <td>-12</td>\n",
              "      <td>0.022</td>\n",
              "      <td>110</td>\n",
              "      <td>0.027</td>\n",
              "      <td>132</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.242</td>\n",
              "      <td>37</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.512</td>\n",
              "      <td>82.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8889</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.681416</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.022</td>\n",
              "      <td>77</td>\n",
              "      <td>0.012</td>\n",
              "      <td>42</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.102</td>\n",
              "      <td>350</td>\n",
              "      <td>0.169</td>\n",
              "      <td>581</td>\n",
              "      <td>0.085</td>\n",
              "      <td>292</td>\n",
              "      <td>0.502582</td>\n",
              "      <td>0.001</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.003431</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.040</td>\n",
              "      <td>137</td>\n",
              "      <td>0.026</td>\n",
              "      <td>90</td>\n",
              "      <td>0.657</td>\n",
              "      <td>79</td>\n",
              "      <td>21</td>\n",
              "      <td>1733.523333</td>\n",
              "      <td>3.9</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.063</td>\n",
              "      <td>215</td>\n",
              "      <td>0.035</td>\n",
              "      <td>119</td>\n",
              "      <td>0.062</td>\n",
              "      <td>213</td>\n",
              "      <td>135</td>\n",
              "      <td>0.028</td>\n",
              "      <td>95</td>\n",
              "      <td>...</td>\n",
              "      <td>0.042</td>\n",
              "      <td>208</td>\n",
              "      <td>0.201</td>\n",
              "      <td>999</td>\n",
              "      <td>0.107</td>\n",
              "      <td>530</td>\n",
              "      <td>0.530531</td>\n",
              "      <td>0.014</td>\n",
              "      <td>70</td>\n",
              "      <td>0.005</td>\n",
              "      <td>25</td>\n",
              "      <td>0.065482</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.061</td>\n",
              "      <td>303</td>\n",
              "      <td>0.047</td>\n",
              "      <td>233</td>\n",
              "      <td>0.769</td>\n",
              "      <td>74</td>\n",
              "      <td>18</td>\n",
              "      <td>2527.658333</td>\n",
              "      <td>6.4</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.006</td>\n",
              "      <td>30</td>\n",
              "      <td>0.048</td>\n",
              "      <td>241</td>\n",
              "      <td>0.027</td>\n",
              "      <td>135</td>\n",
              "      <td>308</td>\n",
              "      <td>0.018</td>\n",
              "      <td>90</td>\n",
              "      <td>0.040</td>\n",
              "      <td>200</td>\n",
              "      <td>0.569</td>\n",
              "      <td>0.251</td>\n",
              "      <td>60</td>\n",
              "      <td>0.769</td>\n",
              "      <td>0.854</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13484</th>\n",
              "      <td>30.0</td>\n",
              "      <td>1.661765</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.029</td>\n",
              "      <td>113</td>\n",
              "      <td>0.022</td>\n",
              "      <td>84</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.115</td>\n",
              "      <td>449</td>\n",
              "      <td>0.258</td>\n",
              "      <td>1005</td>\n",
              "      <td>0.124</td>\n",
              "      <td>484</td>\n",
              "      <td>0.481592</td>\n",
              "      <td>0.001</td>\n",
              "      <td>5</td>\n",
              "      <td>0.001</td>\n",
              "      <td>2</td>\n",
              "      <td>0.004950</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.042</td>\n",
              "      <td>165</td>\n",
              "      <td>0.028</td>\n",
              "      <td>108</td>\n",
              "      <td>0.655</td>\n",
              "      <td>82</td>\n",
              "      <td>40</td>\n",
              "      <td>1991.773333</td>\n",
              "      <td>-3.4</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.025</td>\n",
              "      <td>99</td>\n",
              "      <td>0.035</td>\n",
              "      <td>137</td>\n",
              "      <td>0.036</td>\n",
              "      <td>139</td>\n",
              "      <td>-116</td>\n",
              "      <td>0.012</td>\n",
              "      <td>47</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052</td>\n",
              "      <td>237</td>\n",
              "      <td>0.157</td>\n",
              "      <td>711</td>\n",
              "      <td>0.072</td>\n",
              "      <td>325</td>\n",
              "      <td>0.457103</td>\n",
              "      <td>0.030</td>\n",
              "      <td>136</td>\n",
              "      <td>0.010</td>\n",
              "      <td>45</td>\n",
              "      <td>0.160567</td>\n",
              "      <td>0.331</td>\n",
              "      <td>0.048</td>\n",
              "      <td>217</td>\n",
              "      <td>0.041</td>\n",
              "      <td>184</td>\n",
              "      <td>0.848</td>\n",
              "      <td>77</td>\n",
              "      <td>48</td>\n",
              "      <td>2315.055000</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.008</td>\n",
              "      <td>36</td>\n",
              "      <td>0.049</td>\n",
              "      <td>222</td>\n",
              "      <td>0.030</td>\n",
              "      <td>135</td>\n",
              "      <td>-32</td>\n",
              "      <td>0.011</td>\n",
              "      <td>51</td>\n",
              "      <td>0.024</td>\n",
              "      <td>110</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.207</td>\n",
              "      <td>32</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.450</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10528</th>\n",
              "      <td>25.0</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.010</td>\n",
              "      <td>5</td>\n",
              "      <td>0.039</td>\n",
              "      <td>20</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.085</td>\n",
              "      <td>44</td>\n",
              "      <td>0.115</td>\n",
              "      <td>59</td>\n",
              "      <td>0.049</td>\n",
              "      <td>25</td>\n",
              "      <td>0.423729</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.023</td>\n",
              "      <td>12</td>\n",
              "      <td>0.012</td>\n",
              "      <td>6</td>\n",
              "      <td>0.500</td>\n",
              "      <td>83</td>\n",
              "      <td>15</td>\n",
              "      <td>252.048333</td>\n",
              "      <td>-14.6</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.027</td>\n",
              "      <td>14</td>\n",
              "      <td>0.019</td>\n",
              "      <td>10</td>\n",
              "      <td>0.062</td>\n",
              "      <td>32</td>\n",
              "      <td>-75</td>\n",
              "      <td>0.008</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0.043</td>\n",
              "      <td>137</td>\n",
              "      <td>0.142</td>\n",
              "      <td>450</td>\n",
              "      <td>0.066</td>\n",
              "      <td>207</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.085</td>\n",
              "      <td>268</td>\n",
              "      <td>0.035</td>\n",
              "      <td>111</td>\n",
              "      <td>0.373259</td>\n",
              "      <td>0.414</td>\n",
              "      <td>0.029</td>\n",
              "      <td>91</td>\n",
              "      <td>0.025</td>\n",
              "      <td>79</td>\n",
              "      <td>0.868</td>\n",
              "      <td>79</td>\n",
              "      <td>43</td>\n",
              "      <td>1607.665000</td>\n",
              "      <td>-4.4</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.008</td>\n",
              "      <td>24</td>\n",
              "      <td>0.025</td>\n",
              "      <td>78</td>\n",
              "      <td>0.049</td>\n",
              "      <td>155</td>\n",
              "      <td>-131</td>\n",
              "      <td>0.015</td>\n",
              "      <td>49</td>\n",
              "      <td>0.033</td>\n",
              "      <td>103</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.243</td>\n",
              "      <td>23</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.367</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17253 rows Ã— 232 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       hm_c_AGE  hm_c_AST/TOV  hm_c_AST_PCT  ...  hm_g_2_W_PCT  hm_W_PCT  hm_GP\n",
              "12664      28.0      0.775862         0.063  ...         0.477     0.405   37.0\n",
              "5965       35.0      1.365591         0.107  ...         0.244     0.238   80.0\n",
              "13739      21.0      0.666667         0.056  ...         0.597     0.487   39.0\n",
              "6591       34.0      1.542373         0.070  ...         0.439     0.529   51.0\n",
              "10810      24.0      0.280000         0.020  ...         0.727     0.687   67.0\n",
              "...         ...           ...           ...  ...           ...       ...    ...\n",
              "11823      31.0      1.944444         0.122  ...         0.400     0.327   49.0\n",
              "15436      23.0      2.000000         0.115  ...         0.552     0.512   82.0\n",
              "8889       22.0      0.681416         0.067  ...         0.769     0.854   41.0\n",
              "13484      30.0      1.661765         0.110  ...         0.400     0.450   60.0\n",
              "10528      25.0      0.714286         0.030  ...         0.348     0.367   60.0\n",
              "\n",
              "[17253 rows x 232 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MevLQBh1BmrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stats = pd.read_csv('training_data_mixed_model_v2.csv')\n",
        "bop = pd.read_csv(\"bag_of_players.csv\")\n",
        "df = pd.merge(stats, bop, on=\"game_id\")\n",
        "df = df.drop(\"result_x\", axis = 1)\n",
        "df = df.rename(columns = {\"result_y\":\"result\"})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(\"result\", axis = 1), df[\"result\"], test_size=0.10)\n",
        "hm_train_stats = X_train[[x for x in total_dataset.columns if x.startswith(\"hm_\")]].copy()\n",
        "hm_train_bop = X_train[\"home_players\"].copy()\n",
        "aw_train_stats = X_train[[x for x in total_dataset.columns if x.startswith(\"aw_\")]].copy()\n",
        "aw_train_bop = X_train[\"away_players\"].copy()\n",
        "\n",
        "hm_test_stats = X_test[[x for x in total_dataset.columns if x.startswith(\"hm_\")]].copy()\n",
        "hm_test_bop = X_test[\"home_players\"].copy()\n",
        "aw_test_stats = X_test[[x for x in total_dataset.columns if x.startswith(\"aw_\")]].copy()\n",
        "aw_test_bop = X_test[\"away_players\"].copy()\n"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPte8lD6pO3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_train_examples = len(y_train)\n",
        "num_test_examples = len(y_test)\n",
        "\n",
        "stats_dimension = hm_train_stats.shape[1]\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by -- FOR HOME\n",
        "hm_train_mean = np.mean(hm_train_stats, axis=0)\n",
        "hm_train_std = np.std(hm_train_stats, axis=0)\n",
        "hm_train_stats = (hm_train_stats - hm_train_mean) / hm_train_std\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by -- FOR AWAY\n",
        "aw_train_mean = np.mean(aw_train_stats, axis=0)\n",
        "aw_train_std = np.std(aw_train_mean, axis=0)\n",
        "aw_train_stats = (aw_train_stats - aw_train_mean) / aw_train_std"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-k_gPJaWlWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86bf4ff1-8ef1-4396-dc5e-1cfc6e05d789"
      },
      "source": [
        "aw_train_stats.shape[1]"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "232"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kc8T9X5KpO4E"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model. First we stack a few layers together using `keras.Sequential`. Next we configure the loss function, optimizer, and metrics to monitor. These are added during the model's compile step:\n",
        "\n",
        "* *Loss function* - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
        "* *Optimizer* - how the model is updated based on the data it sees and its loss function.\n",
        "* *Metrics* - used to monitor the training and testing steps.\n",
        "\n",
        "Let's build a network with 1 hidden layer of 20 neurons, and use mean squared error (MSE) as the loss function (most common one for regression problems):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YD2ejt8PpO4H"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the modelâ€”in this example, the `train_features` and `train_labels` arrays.\n",
        "2. The model learns to associate features and labels.\n",
        "3. We ask the model to make predictions about a test setâ€”in this example, the `test_features` array. We verify that the predictions match the labels from the `test_labels` array. \n",
        "\n",
        "To start training,  call the `model.fit` methodâ€”the model is \"fit\" to the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1qRUT_Qi9tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def lst_removewspc(lst):\n",
        "  x = []\n",
        "  for st in lst:\n",
        "    st = st.replace(\" \", \"\")\n",
        "    st = st.replace(\".\", \"\")\n",
        "    st = st.replace(\"-\", \"\")\n",
        "    x.append(st)\n",
        "  return x\n",
        "\n",
        "hm_train_bop = hm_train_bop.apply(lambda x: literal_eval(str(x)))\n",
        "aw_train_bop = aw_train_bop.apply(lambda x: literal_eval(str(x)))\n",
        "hm_train_bop = hm_train_bop.apply(lambda x: lst_removewspc(x))\n",
        "aw_train_bop = aw_train_bop.apply(lambda x: lst_removewspc(x))\n",
        "\n",
        "playernames_set = set()\n",
        "for index,value in hm_train_bop.iteritems():\n",
        "  playernames_set.update(value)\n",
        "for index,value in aw_train_bop.iteritems():\n",
        "  playernames_set.update(row.away_players)\n",
        "\n",
        "VOCAB_SIZE = len(playernames_set)\n",
        "encoded_home = []\n",
        "for index,value in hm_train_bop.iteritems():\n",
        "  concat_string = \" \".join(value)\n",
        "  encoded_home.append(one_hot(concat_string,VOCAB_SIZE))\n",
        "encoded_away = []\n",
        "for index,value in aw_train_bop.iteritems():\n",
        "  concat_string = \" \".join(value)\n",
        "  encoded_away.append(one_hot(concat_string,VOCAB_SIZE))\n",
        "\n",
        "MAX_LENGTH = 5\n",
        "padded_hm_encodings = pad_sequences(encoded_home, maxlen=MAX_LENGTH, padding = \"post\", truncating = \"post\")\n",
        "padded_aw_encodings = pad_sequences(encoded_away, maxlen=MAX_LENGTH, padding = \"post\", truncating = \"post\")"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqMe1ojSCFO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp_hm_stats = tf.keras.Input(shape=(hm_train_stats.shape[1],), name = \"home_stats\")\n",
        "inp_hm_bop = tf.keras.Input(shape=(padded_hm_encodings.shape[1],), name = \"home_bag_of_players\")\n",
        "inp_aw_stats = tf.keras.Input(shape=(aw_train_stats.shape[1],), name = \"away_stats\")\n",
        "inp_aw_bop = tf.keras.Input(shape=(padded_aw_encodings.shape[1],), name = \"away_bag_of_players\")\n",
        "\n",
        "#home stats\n",
        "dense_hm_stats = tf.keras.layers.Dense(ceil(1/3*X_train.shape[1]), kernel_regularizer=\"l1\", name = \"dense_for_home_stats\")(inp_hm_stats)\n",
        "lky_hm_stats = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_hm_stats)\n",
        "\n",
        "#home bag of players\n",
        "embedding_hm = tf.keras.layers.Embedding(VOCAB_SIZE, 8, input_length = MAX_LENGTH)(inp_hm_bop)\n",
        "flatten_hm = tf.keras.layers.Flatten()(embedding_hm)\n",
        "\n",
        "#home together\n",
        "home_merged = tf.keras.layers.concatenate([flatten_hm, lky_hm_stats], axis=1, name = \"home_merged\")\n",
        "dropout_hm_merged = tf.keras.layers.Dropout(0.25, name = \"dropout_hm_merged\")(home_merged)\n",
        "dense_hm = tf.keras.layers.Dense(ceil(sqrt(1/3*X_train.shape[1])), kernel_regularizer=\"l2\", name = \"dense_all_home\")(dropout_hm_merged)\n",
        "lky_hm = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_hm)\n",
        "\n",
        "#away stats\n",
        "dense_aw_stats = tf.keras.layers.Dense(ceil(1/3*X_train.shape[1]), kernel_regularizer=\"l1\", name = \"dense_for_away_stats\")(inp_aw_stats)\n",
        "lky_aw_stats = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_aw_stats)\n",
        "\n",
        "#away bag of players\n",
        "embedding_aw = tf.keras.layers.Embedding(VOCAB_SIZE, 8, input_length = MAX_LENGTH)(inp_aw_bop)\n",
        "flatten_aw = tf.keras.layers.Flatten()(embedding_aw)\n",
        "\n",
        "#away together\n",
        "away_merged = tf.keras.layers.concatenate([flatten_aw, lky_aw_stats], axis=1, name = \"away_merged\")\n",
        "dropout_aw_merged = tf.keras.layers.Dropout(0.25, name = \"dropout_aw_merged\")(away_merged)\n",
        "dense_aw = tf.keras.layers.Dense(ceil(sqrt(1/3*X_train.shape[1])), kernel_regularizer=\"l2\", name = \"dense_all_away\")(dropout_aw_merged)\n",
        "lky_aw = tf.keras.layers.LeakyReLU(alpha=0.2)(dense_aw)\n",
        "\n",
        "#all together\n",
        "merged = tf.keras.layers.concatenate([lky_hm, lky_aw], axis=1, name = \"all_merged\")\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid', name = \"output\")(merged)\n",
        "\n",
        "model = keras.Model(inputs = [inp_hm_stats, inp_hm_bop, inp_aw_stats, inp_aw_bop], outputs = output)"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVCc7mGKeKO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "bbb55157-157c-4d0c-dd6e-59a1778adc2a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "home_bag_of_players (InputLayer [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "home_stats (InputLayer)         [(None, 232)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "away_bag_of_players (InputLayer [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "away_stats (InputLayer)         [(None, 232)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_39 (Embedding)        (None, 5, 8)         9368        home_bag_of_players[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_for_home_stats (Dense)    (None, 156)          36348       home_stats[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "embedding_40 (Embedding)        (None, 5, 8)         9368        away_bag_of_players[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_for_away_stats (Dense)    (None, 156)          36348       away_stats[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_39 (Flatten)            (None, 40)           0           embedding_39[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_76 (LeakyReLU)      (None, 156)          0           dense_for_home_stats[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_40 (Flatten)            (None, 40)           0           embedding_40[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_78 (LeakyReLU)      (None, 156)          0           dense_for_away_stats[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "home_merged (Concatenate)       (None, 196)          0           flatten_39[0][0]                 \n",
            "                                                                 leaky_re_lu_76[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "away_merged (Concatenate)       (None, 196)          0           flatten_40[0][0]                 \n",
            "                                                                 leaky_re_lu_78[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_hm_merged (Dropout)     (None, 196)          0           home_merged[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_aw_merged (Dropout)     (None, 196)          0           away_merged[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_all_home (Dense)          (None, 13)           2561        dropout_hm_merged[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_all_away (Dense)          (None, 13)           2561        dropout_aw_merged[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_77 (LeakyReLU)      (None, 13)           0           dense_all_home[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_79 (LeakyReLU)      (None, 13)           0           dense_all_away[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "all_merged (Concatenate)        (None, 26)           0           leaky_re_lu_77[0][0]             \n",
            "                                                                 leaky_re_lu_79[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            27          all_merged[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 96,581\n",
            "Trainable params: 96,581\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w38HxaFhg00Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "b04e9c13-0084-4135-99d5-bfcfc66ab33c"
      },
      "source": [
        "model.compile(optimizer=\"adam\", \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights = True)\n",
        "\n",
        "history = model.fit([np.array(hm_train_stats), np.array(padded_hm_encodings), np.array(aw_train_stats), np.array(padded_aw_encodings)], y_train, epochs=1000, verbose=1, validation_split = 0.1,\n",
        "                    callbacks=[early_stop], shuffle = True, batch_size = 32)\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "acc = float(hist['val_accuracy'].tail(1))\n",
        "max_acc = float(max(list(hist['val_accuracy'])))\n",
        "print()\n",
        "print('Max accuracy on validation set: {}'.format(round(max_acc, 3)))"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 5.0967 - accuracy: 0.6820 - val_loss: 0.7621 - val_accuracy: 0.7039\n",
            "Epoch 2/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.7438 - accuracy: 0.6994 - val_loss: 0.7272 - val_accuracy: 0.7178\n",
            "Epoch 3/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.7177 - accuracy: 0.7097 - val_loss: 0.7228 - val_accuracy: 0.7074\n",
            "Epoch 4/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.7026 - accuracy: 0.7146 - val_loss: 0.7121 - val_accuracy: 0.7103\n",
            "Epoch 5/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6926 - accuracy: 0.7183 - val_loss: 0.7155 - val_accuracy: 0.6912\n",
            "Epoch 6/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6808 - accuracy: 0.7244 - val_loss: 0.7093 - val_accuracy: 0.7016\n",
            "Epoch 7/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6746 - accuracy: 0.7286 - val_loss: 0.7158 - val_accuracy: 0.6958\n",
            "Epoch 8/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6694 - accuracy: 0.7310 - val_loss: 0.7262 - val_accuracy: 0.6831\n",
            "Epoch 9/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6653 - accuracy: 0.7293 - val_loss: 0.7189 - val_accuracy: 0.6924\n",
            "Epoch 10/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6587 - accuracy: 0.7390 - val_loss: 0.7318 - val_accuracy: 0.6906\n",
            "Epoch 11/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6593 - accuracy: 0.7386 - val_loss: 0.7310 - val_accuracy: 0.6895\n",
            "Epoch 12/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6549 - accuracy: 0.7407 - val_loss: 0.7197 - val_accuracy: 0.6889\n",
            "Epoch 13/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6481 - accuracy: 0.7391 - val_loss: 0.7224 - val_accuracy: 0.6952\n",
            "Epoch 14/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6464 - accuracy: 0.7417 - val_loss: 0.7306 - val_accuracy: 0.6889\n",
            "Epoch 15/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6491 - accuracy: 0.7435 - val_loss: 0.7287 - val_accuracy: 0.6784\n",
            "Epoch 16/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6445 - accuracy: 0.7456 - val_loss: 0.7409 - val_accuracy: 0.6825\n",
            "Epoch 17/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6415 - accuracy: 0.7472 - val_loss: 0.7272 - val_accuracy: 0.6837\n",
            "Epoch 18/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6494 - accuracy: 0.7443 - val_loss: 0.7482 - val_accuracy: 0.6808\n",
            "Epoch 19/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6440 - accuracy: 0.7482 - val_loss: 0.7352 - val_accuracy: 0.6912\n",
            "Epoch 20/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6430 - accuracy: 0.7457 - val_loss: 0.7302 - val_accuracy: 0.6866\n",
            "Epoch 21/1000\n",
            "486/486 [==============================] - 2s 4ms/step - loss: 0.6454 - accuracy: 0.7434 - val_loss: 0.7443 - val_accuracy: 0.6866\n",
            "\n",
            "Final accuracy on validation set: 0.687\n",
            "Max accuracy on validation set: 0.718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BuBdXr_9pO4H",
        "colab": {}
      },
      "source": [
        "# this helps makes our output less verbose but still shows progress\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0: print('')\n",
        "        print('.', end='')\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "        Dense(ceil(2/3*X_train.shape[1]), kernel_regularizer=\"l1\"),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.1),\n",
        "        Dense(ceil(sqrt(2/3*X_train.shape[1])), kernel_regularizer=\"l2\"),\n",
        "        LeakyReLU(alpha=0.5),\n",
        "        Dense(1, activation = \"sigmoid\")\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights = True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=1000, verbose=1, validation_split = 0.1,\n",
        "                    callbacks=[early_stop], shuffle = True, batch_size = 128)\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "acc = float(hist['val_accuracy'].tail(1))\n",
        "max_acc = float(max(list(hist['val_accuracy'])))\n",
        "print()\n",
        "print('Final accuracy on validation set: {}'.format(round(acc, 3)))\n",
        "print('Max accuracy on validation set: {}'.format(round(max_acc, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8B-aVPapO4K",
        "colab_type": "text"
      },
      "source": [
        "Now, let's plot the loss function measure on the training and validation sets. The validation set is used to prevent overfitting ([learn more about it here](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)). However, because our network is small, the training convergence without noticeably overfitting the data as the plot shows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1mUw8Ha8pO4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "461dc7bb-de81-4229-85f5-32435940fd78"
      },
      "source": [
        "def plot_history():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "plot_history()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W2ALGSGMpO4N"
      },
      "source": [
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dXvJAoKspO4O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "420c08b9-5062-4013-e318-397b1ddce5e0"
      },
      "source": [
        "test_features_norm = (X_test - train_mean) / train_std\n",
        "loss, accuracy = model.evaluate(test_features_norm, y_test)\n",
        "print('Accuracy on test set: {}'.format(round(accuracy, 3)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 0s 3ms/step - loss: 0.7391 - accuracy: 0.7204\n",
            "Accuracy on test set: 0.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xL669VC-pO4p"
      },
      "source": [
        "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 98.68% on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NIUd4T1fpO4t"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "With the model trained, we can use it to make predictions about some games.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOz_TewXpO4z",
        "colab_type": "text"
      },
      "source": [
        "## Acknowledgements\n",
        "\n",
        "The contents of this tutorial is based on and inspired by the work of [TensorFlow team](https://www.tensorflow.org) (see their [Colab notebooks](https://www.tensorflow.org/tutorials/)), our [MIT Human-Centered AI team](https://hcai.mit.edu), and individual pieces referenced in the [MIT Deep Learning](https://deeplearning.mit.edu) course slides."
      ]
    }
  ]
}